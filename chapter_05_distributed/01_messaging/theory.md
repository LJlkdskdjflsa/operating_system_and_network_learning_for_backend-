# Messaging Patterns

## Overview

Messaging enables asynchronous communication between components. Understanding messaging patterns is essential for building scalable, decoupled systems.

## Why Messaging?

### Direct Communication (Tight Coupling)
```
Service A --HTTP--> Service B
           ↓
    If B is slow or down, A blocks or fails
```

### Message-Based (Loose Coupling)
```
Service A --> Queue --> Service B
     ↓
  A doesn't wait
  B processes when ready
  If B is down, messages wait in queue
```

## Channel Types in Rust

### oneshot - Single Value

```rust
use tokio::sync::oneshot;

// Create channel
let (tx, rx) = oneshot::channel();

// Send single value
tx.send("done").unwrap();

// Receive
let result = rx.await.unwrap();
```

Use case: Request-response, task completion notification

### mpsc - Multi-Producer, Single Consumer

```rust
use tokio::sync::mpsc;

// Create bounded channel
let (tx, mut rx) = mpsc::channel(100);

// Multiple producers
let tx2 = tx.clone();
tokio::spawn(async move { tx.send("from 1").await });
tokio::spawn(async move { tx2.send("from 2").await });

// Single consumer
while let Some(msg) = rx.recv().await {
    println!("{}", msg);
}
```

Use case: Job queues, logging, fan-in pattern

### broadcast - Multi-Producer, Multi-Consumer

```rust
use tokio::sync::broadcast;

// Create channel
let (tx, _rx) = broadcast::channel(100);

// Multiple subscribers
let mut rx1 = tx.subscribe();
let mut rx2 = tx.subscribe();

// Send (all subscribers receive)
tx.send("event").unwrap();

// Each receives independently
let msg1 = rx1.recv().await.unwrap();
let msg2 = rx2.recv().await.unwrap();
```

Use case: Event broadcasting, pub/sub

### watch - Single Value, Multiple Observers

```rust
use tokio::sync::watch;

// Create with initial value
let (tx, rx) = watch::channel("initial");

// Multiple observers
let mut rx2 = rx.clone();

// Update value
tx.send("updated").unwrap();

// Observers see latest value
println!("{}", *rx.borrow());
```

Use case: Configuration updates, state broadcasting

## Messaging Patterns

### Fan-Out (One to Many)

```
Producer --> Broadcast Channel --> Consumer 1
                              --> Consumer 2
                              --> Consumer 3
```

```rust
let (tx, _) = broadcast::channel(100);

// Spawn consumers
for i in 0..3 {
    let mut rx = tx.subscribe();
    tokio::spawn(async move {
        while let Ok(msg) = rx.recv().await {
            println!("Consumer {}: {}", i, msg);
        }
    });
}

// Producer sends to all
tx.send("message").unwrap();
```

In your Tokio broadcast example:

- broadcast::channel(100) creates a broadcast channel with a buffer of 100.
- Each tx.subscribe() gets its own independent receiver.
- After tx.send("message"), all subscribers receive the message.

Common use cases:

- Event notifications: e.g., order status updates that notify multiple services (notifications, analytics, cache refresh).
- Multi-sink logging/monitoring: the same event stream written to logs, metrics, and alert systems.
- Real-time push: chat, market data, announcements broadcast to many connections.
- State sync: config changes or feature flag updates propagated to multiple nodes.


### Fan-In (Many to One)

```
Producer 1 --> mpsc Channel --> Consumer
Producer 2 -->
Producer 3 -->
```

```rust
let (tx, mut rx) = mpsc::channel(100);

// Spawn producers
for i in 0..3 {
    let tx = tx.clone();
    tokio::spawn(async move {
        tx.send(format!("from producer {}", i)).await.unwrap();
    });
}
drop(tx); // Drop original sender

// Single consumer
while let Some(msg) = rx.recv().await {
    println!("{}", msg);
}
```

In the Tokio mpsc example:

- mpsc::channel(100) creates a multi‑producer, single‑consumer channel with buffer 100.
- Each producer clones tx and sends a message.
- drop(tx) closes the original sender so the consumer can exit once all producers finish.
- The consumer loops on rx.recv().await and handles messages in arrival order.

Use cases:

- Aggregating work from multiple sources into one worker (e.g., log ingestion, metrics aggregation).
- Merging event streams from different components into a single processing pipeline.
- Task dispatch: many tasks generated by different modules, processed by one coordinator.
- Backpressure control: a single consumer can regulate overall throughput via channel capacity.



### Worker Pool

```
         ┌─> Worker 1 ─┐
Jobs --> │   Worker 2  │ --> Results
         └─> Worker 3 ─┘
```

```rust
let (job_tx, job_rx) = async_channel::bounded(100);
let (result_tx, mut result_rx) = mpsc::channel(100);

// Spawn workers
for id in 0..4 {
    let rx = job_rx.clone();
    let tx = result_tx.clone();
    tokio::spawn(async move {
        while let Ok(job) = rx.recv().await {
            let result = process(job);
            tx.send((id, result)).await.unwrap();
        }
    });
}

// Submit jobs
for job in jobs {
    job_tx.send(job).await.unwrap();
}

// Collect results
while let Some((worker_id, result)) = result_rx.recv().await {
    println!("Worker {} completed: {:?}", worker_id, result);
}
```

Worker Pool means “multiple workers pull jobs from the same task queue and process them in parallel, then send results back to a single results channel.”

What the code is doing:

- async_channel::bounded(100) creates the job queue; all workers share job_rx.
- Each worker calls job_rx.recv(), processes one job, and sends the result to result_tx.
- The main thread sends jobs into job_tx, then collects results from result_rx.
- This achieves load balancing: each job is handled by only one worker.

Common use cases:

- Distributing CPU/IO‑heavy work (image processing, transcoding, web crawling, batch API calls).
- Background job handling (email sending, report generation, data cleanup).
- Improving throughput and stabilizing latency by avoiding a single consumer bottleneck.
- Controlled concurrency: the number of workers caps how many tasks run at once.

## Backpressure

When producer is faster than consumer:

### Unbounded Channel (Dangerous)

```rust
// Memory can grow without limit!
let (tx, rx) = mpsc::unbounded_channel();
```

### Bounded Channel (Safe)

```rust
// Blocks when full
let (tx, rx) = mpsc::channel(100);

// Or use try_send to avoid blocking
match tx.try_send(msg) {
    Ok(()) => { /* sent */ }
    Err(TrySendError::Full(_)) => { /* handle backpressure */ }
    Err(TrySendError::Closed(_)) => { /* channel closed */ }
}
```

### Backpressure Strategies

1. **Block**: Wait until space available (default)
2. **Drop**: Discard message on full
3. **Replace**: Overwrite oldest message
4. **Error**: Return error to caller

## Message Delivery Guarantees

### At-Most-Once

```rust
// Send and forget
let _ = tx.send(msg);  // Ignoring result
```

- May lose messages
- No duplicates
- Lowest latency

### At-Least-Once

```rust
// Retry until acknowledged
loop {
    tx.send(msg.clone()).await?;
    match rx.recv_timeout(Duration::from_secs(5)).await {
        Ok(ack) => break,
        Err(_) => continue,  // Retry
    }
}
```

- No message loss
- May have duplicates
- Requires idempotent processing

### Exactly-Once

Requires:
- Deduplication (track message IDs)
- Idempotent operations
- Transactional processing

## Simple Message Queue Design

```rust
struct Message {
    id: Uuid,
    payload: String,
    created_at: Instant,
    attempts: u32,
}

struct Queue {
    pending: VecDeque<Message>,
    processing: HashMap<Uuid, Message>,
    visibility_timeout: Duration,
}

impl Queue {
    fn enqueue(&mut self, payload: String) -> Uuid {
        let msg = Message {
            id: Uuid::new_v4(),
            payload,
            created_at: Instant::now(),
            attempts: 0,
        };
        let id = msg.id;
        self.pending.push_back(msg);
        id
    }

    fn dequeue(&mut self) -> Option<Message> {
        let mut msg = self.pending.pop_front()?;
        msg.attempts += 1;
        self.processing.insert(msg.id, msg.clone());
        Some(msg)
    }

    fn acknowledge(&mut self, id: Uuid) -> bool {
        self.processing.remove(&id).is_some()
    }

    fn check_timeouts(&mut self) {
        let now = Instant::now();
        let expired: Vec<_> = self.processing
            .iter()
            .filter(|(_, m)| now.duration_since(m.created_at) > self.visibility_timeout)
            .map(|(id, _)| *id)
            .collect();

        for id in expired {
            if let Some(msg) = self.processing.remove(&id) {
                self.pending.push_back(msg);
            }
        }
    }
}
```
This is an in‑memory message queue with visibility timeout and at‑least‑once delivery.

Message holds the message data plus metadata: id, created_at, and attempts.
Queue keeps two sets:
- pending: messages waiting to be delivered.
- processing: messages that have been delivered but not yet acknowledged.
- enqueue creates a message and pushes it to pending, returning its ID.
- dequeue pops one from pending, increments attempts, moves it into processing, and returns it.
- acknowledge removes a message from processing when the consumer confirms success.
- check_timeouts scans processing for messages whose visibility timeout has expired and requeues them back to pending.

Behaviorally:

- A consumer gets a message via dequeue.
- If it finishes, it calls acknowledge.
- If it crashes or takes too long, check_timeouts makes the message visible again, so another consumer can retry.

## Graceful Shutdown

```rust
use tokio::sync::mpsc;
use tokio_util::sync::CancellationToken;

async fn worker(
    mut rx: mpsc::Receiver<Job>,
    cancel: CancellationToken,
) {
    loop {
        tokio::select! {
            Some(job) = rx.recv() => {
                process(job).await;
            }
            _ = cancel.cancelled() => {
                // Finish current work
                while let Ok(job) = rx.try_recv() {
                    process(job).await;
                }
                break;
            }
        }
    }
}

// Shutdown
cancel.cancel();
worker_handle.await.unwrap();
```

Graceful shutdown lets you stop a service without losing work or corrupting state.

- Prevents in‑flight jobs from being dropped halfway through.
- Ensures acknowledgements/commits happen, avoiding duplicate or lost messages.
- Lets you flush buffers, close connections, and release locks cleanly.
- Keeps state consistent (files, DB transactions, caches).
- Makes restarts/rollouts safer with predictable behavior.

---

This sample shows a worker that can shut down gracefully using a cancellation token.

- The worker loops with tokio::select! and waits for either:
  - a job from rx.recv(), or
  - a cancellation signal from cancel.cancelled().
- If it receives a job, it processes it normally.
- If cancellation happens, it drains any already‑queued jobs with try_recv() and processes them, then breaks out of the loop.
- Shutdown is triggered by cancel.cancel(), and the caller waits for the worker to finish with worker_handle.await.
- Effect: once shutdown starts, the worker stops waiting for new jobs but still finishes queued work before exiting.

---

tokio::select! waits on multiple async operations at the same time and runs the branch for the one that becomes ready first, then cancels the others for that loop iteration.

Key points:

- It’s like an async “race” between futures.
- Only the first ready branch runs; others are dropped (unless you use biased;).
- Great for handling “whichever happens first” (e.g., job vs. shutdown).

## Real-World Message Queues

### Redis (Simple)
- LPUSH/RPOP for basic queues
- BLPOP for blocking
- Pub/Sub for broadcast

### RabbitMQ
- AMQP protocol
- Acknowledgments
- Dead letter queues
- Routing

### Kafka
- Distributed log
- Partitioning
- Consumer groups
- High throughput

## Summary

- **Channels** enable async communication
- **mpsc** for fan-in, **broadcast** for fan-out
- **Bounded channels** prevent memory exhaustion
- **At-least-once** delivery needs idempotency
- **Graceful shutdown** handles in-flight messages

## Labs

1. **Lab 1: Channel Patterns** - Fan-in, fan-out, worker pool
2. **Lab 2: Simple Queue** - Message queue with acknowledgment
